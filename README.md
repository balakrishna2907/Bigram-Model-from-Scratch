# Bigram Model from Scratch

This repository contains a self-project in which I implemented a neural network-based language model from scratch to predict the next character using bigram probabilities. The project leverages batch processing to handle large datasets efficiently, enabling faster training. Additionally, a custom batch normalization layer was integrated into the neural network training pipeline to stabilize the learning process, accelerate convergence, and improve predictive accuracy.

## Features

- **Bigram Language Model**: Implements a neural network-based language model that predicts the next character in a sequence using bigram probabilities.
- **Batch Processing**: Utilizes batch processing to efficiently handle large datasets, speeding up training and reducing memory consumption.
- **Batch Normalization**: A custom batch normalization layer integrated into the model to stabilize the learning process, improve training speed, and enhance model accuracy.
- **Efficient Training Pipeline**: Designed for scalability, allowing training on larger datasets without a significant drop in performance.

